{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgZ8Vm8Br1f5NzIfipdRGU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Edenshmuel/ICU_Nutrition_ML/blob/main/Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This notebook defines the preprocessing pipeline for both clustering and prediction models.\n",
        "It includes transformations for numerical, categorical, and skewed features**"
      ],
      "metadata": {
        "id": "tlrGG3q-Mw8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Necessary Libraries"
      ],
      "metadata": {
        "id": "zwky4Y8NMfk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder"
      ],
      "metadata": {
        "id": "KM2K6RqGJhZ1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Log Transform + Scaling for skewed features"
      ],
      "metadata": {
        "id": "pEPQEqtXJS2U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wxbw2hxHI9yX"
      },
      "outputs": [],
      "source": [
        "log_scaler_pipeline = Pipeline(steps=[(\"log_transform\", FunctionTransformer(np.log1p, validate=True)),\n",
        "    (\"scaler\", MinMaxScaler())])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standard Scaling for non-skewed features"
      ],
      "metadata": {
        "id": "1FIOmR_sJa2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler_pipeline = Pipeline(steps=[(\"scaler\", MinMaxScaler())])"
      ],
      "metadata": {
        "id": "HFJrHPTBJV7e"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-Hot Encoding for categorical features"
      ],
      "metadata": {
        "id": "mz0H0btiJxt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_transformer = Pipeline(steps=[(\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))])"
      ],
      "metadata": {
        "id": "CridlTu3JaSW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function converts the \"Disease\" column, which contains multiple diseases as a comma-separated string, into a multi-hot encoded formatcreating a separate binary column for each unique disease"
      ],
      "metadata": {
        "id": "b9ee6DAHJ3Ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_hot_encode_disease(df):\n",
        "    df = df.copy()\n",
        "    df[\"Disease\"] = df[\"Disease\"].astype(str).str.split(\", \")\n",
        "    all_diseases = set([d for sublist in df[\"Disease\"] for d in sublist])\n",
        "\n",
        "    for disease in all_diseases:\n",
        "        df[disease] = df[\"Disease\"].apply(lambda x: 1 if disease in x else 0)\n",
        "\n",
        "    df = df.drop(columns=[\"Disease\"])\n",
        "    return df\n",
        "\n",
        "disease_transformer = FunctionTransformer(multi_hot_encode_disease)"
      ],
      "metadata": {
        "id": "UhDGu3sPJxJk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code transforms the categorical \"Activity Level\" column into numerical values, making it suitable for machine learning models"
      ],
      "metadata": {
        "id": "-qZ8hdfrJ9bB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "activity_mapping = {\n",
        "    \"Sedentary\": 0,\n",
        "    \"Lightly Active\": 1,\n",
        "    \"Moderately Active\": 2,\n",
        "    \"Very Active\": 3,\n",
        "    \"Extremely Active\": 4\n",
        "    }"
      ],
      "metadata": {
        "id": "md4_CXUhJ6Tv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_activity_level(X):\n",
        "    X = X.copy()\n",
        "    X[\"Activity Level\"] = X[\"Activity Level\"].map(activity_mapping)\n",
        "    return X\n",
        "\n",
        "activity_transformer = FunctionTransformer(encode_activity_level)"
      ],
      "metadata": {
        "id": "jWO1NIblKDNB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class is a custom scikit-learn transformer that calculates the Body Mass Index (BMI) based on weight and height"
      ],
      "metadata": {
        "id": "-A371_gvKIYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BMICalculator(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X[\"BMI\"] = X[\"Weight\"] / (X[\"Height\"] ** 2)\n",
        "        return X"
      ],
      "metadata": {
        "id": "tAKarD-hKGR4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Preprocessing Pipeline"
      ],
      "metadata": {
        "id": "81aYHNAVKWe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_pipeline = Pipeline(steps=[\n",
        "    (\"bmi_calculator\", BMICalculator()),\n",
        "    (\"log_scaled\", log_scaler_pipeline),\n",
        "    (\"scaler\", scaler_pipeline)])"
      ],
      "metadata": {
        "id": "MxoSVCHBKKhy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_preprocessor(numerical_features, categorical_features, Multy_categorical_features, right_skewed_features=None):\n",
        "    transformers = []\n",
        "\n",
        "    if right_skewed_features:\n",
        "        transformers.append((\"log_scaled\", log_scaler_pipeline, right_skewed_features))\n",
        "\n",
        "    transformers.extend([\n",
        "        (\"scaled\", scaler_pipeline, numerical_features),\n",
        "        (\"activity\", activity_transformer, [\"Activity Level\"]),\n",
        "        (\"cat\", cat_transformer, categorical_features),\n",
        "        (\"disease\", disease_transformer, [\"Disease\"])])\n",
        "\n",
        "    preprocessor = ColumnTransformer(transformers=transformers)\n",
        "\n",
        "    return preprocessor"
      ],
      "metadata": {
        "id": "2Q3YjGxKKZiv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_feature_names(preprocessor, input_features):\n",
        "    \"\"\" 专 转 砖转 注转 专 专住驻专爪 砖 ColumnTransformer \"\"\"\n",
        "    feature_names = []\n",
        "\n",
        "    for name, transformer, columns in preprocessor.transformers_:\n",
        "        if transformer == \"passthrough\":\n",
        "            # 专 砖 拽住 砖转 拽专 拽专 砖 passthrough\n",
        "            if isinstance(columns[0], int):\n",
        "                feature_names.extend([input_features[i] for i in columns])\n",
        "            else:\n",
        "                feature_names.extend(columns)\n",
        "\n",
        "        elif isinstance(transformer, OneHotEncoder):\n",
        "            # 注专 OneHotEncoder, 砖砖 -get_feature_names_out()\n",
        "            try:\n",
        "                ohe_feature_names = transformer.get_feature_names_out(columns)\n",
        "                feature_names.extend(ohe_feature_names)\n",
        "            except:\n",
        "                feature_names.extend(columns)\n",
        "\n",
        "        elif isinstance(transformer, Pipeline):\n",
        "            #   Pipeline, 住 拽转 转 砖转 砖 专 ( 驻砖专)\n",
        "            last_step = transformer.steps[-1][1]\n",
        "            if hasattr(last_step, \"get_feature_names_out\"):\n",
        "                try:\n",
        "                    feature_names.extend(last_step.get_feature_names_out(columns))\n",
        "                except:\n",
        "                    feature_names.extend(columns)\n",
        "            else:\n",
        "                feature_names.extend(columns)\n",
        "\n",
        "        elif hasattr(transformer, \"get_feature_names_out\"):\n",
        "            #  专住驻专专 转 -get_feature_names_out()\n",
        "            try:\n",
        "                feature_names.extend(transformer.get_feature_names_out(columns))\n",
        "            except:\n",
        "                feature_names.extend(columns)\n",
        "\n",
        "        else:\n",
        "            #   专 砖 转 砖转, 砖专 转 拽专\n",
        "            feature_names.extend(columns)\n",
        "\n",
        "    #  转拽 住祝:  注 砖 住驻专, 专 转 砖转 拽专\n",
        "    feature_names = [input_features[i] if isinstance(i, int) else i for i in feature_names]\n",
        "\n",
        "    return feature_names\n",
        "\n"
      ],
      "metadata": {
        "id": "ltG5gBmaFW7l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}